{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spotify Data Cleaning & Processing\n",
        "\n",
        "This notebook prepares the three raw datasets for visualization:\n",
        "- `charts.csv` (daily regional charts)\n",
        "- `Most Streamed Spotify Songs 2024.csv` (cross-platform performance)\n",
        "- `tracks.csv` (audio features & genres)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89e1a0fc",
      "metadata": {},
      "source": [
        "## 1. Load raw datasets\n",
        "\n",
        "In this section we load the three CSV files and do very light inspection (shapes, columns, sample rows). We'll add cleaning/processing in later sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "87ee82c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\muhammad bilal\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\muhammad bilal\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\muhammad bilal\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\muhammad bilal\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\muhammad bilal\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\muhammad bilal\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "DATA_DIR: C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\data\\raw\n",
            "Exists: True\n",
            "Contents: [WindowsPath('C:/Users/Muhammad Bilal/Desktop/dataviz 1/viz/data/raw/charts.csv'), WindowsPath('C:/Users/Muhammad Bilal/Desktop/dataviz 1/viz/data/raw/Most Streamed Spotify Songs 2024.csv'), WindowsPath('C:/Users/Muhammad Bilal/Desktop/dataviz 1/viz/data/raw/tracks.csv')]\n",
            "PROCESSED_DIR: C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\data\\processed\n",
            "Reading C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\data\\raw\\charts.csv ...\n",
            "Reading C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\data\\raw\\Most Streamed Spotify Songs 2024.csv ...\n",
            "Reading C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\data\\raw\\tracks.csv ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(                         title  rank        date  \\\n",
              " 0      Chantaje (feat. Maluma)     1  2017-01-01   \n",
              " 1  Vente Pa' Ca (feat. Maluma)     2  2017-01-01   \n",
              " 2   Reggaetón Lento (Bailemos)     3  2017-01-01   \n",
              " 3                       Safari     4  2017-01-01   \n",
              " 4                  Shaky Shaky     5  2017-01-01   \n",
              " \n",
              "                                   artist  \\\n",
              " 0                                Shakira   \n",
              " 1                           Ricky Martin   \n",
              " 2                                   CNCO   \n",
              " 3  J Balvin, Pharrell Williams, BIA, Sky   \n",
              " 4                           Daddy Yankee   \n",
              " \n",
              "                                                  url     region   chart  \\\n",
              " 0  https://open.spotify.com/track/6mICuAdrwEjh6Y6...  Argentina  top200   \n",
              " 1  https://open.spotify.com/track/7DM4BPaS7uofFul...  Argentina  top200   \n",
              " 2  https://open.spotify.com/track/3AEZUABDXNtecAO...  Argentina  top200   \n",
              " 3  https://open.spotify.com/track/6rQSrBHf7HlZjtc...  Argentina  top200   \n",
              " 4  https://open.spotify.com/track/58IL315gMSTD37D...  Argentina  top200   \n",
              " \n",
              "            trend  streams  \n",
              " 0  SAME_POSITION   253019  \n",
              " 1        MOVE_UP   223988  \n",
              " 2      MOVE_DOWN   210943  \n",
              " 3  SAME_POSITION   173865  \n",
              " 4        MOVE_UP   153956  ,\n",
              "                         Track                    Album Name          Artist  \\\n",
              " 0         MILLION DOLLAR BABY  Million Dollar Baby - Single   Tommy Richman   \n",
              " 1                 Not Like Us                   Not Like Us  Kendrick Lamar   \n",
              " 2  i like the way you kiss me    I like the way you kiss me         Artemas   \n",
              " 3                     Flowers              Flowers - Single     Miley Cyrus   \n",
              " 4                     Houdini                       Houdini          Eminem   \n",
              " \n",
              "   Release Date          ISRC  All Time Rank  Track Score Spotify Streams  \\\n",
              " 0    4/26/2024  QM24S2402528              1        725.4     390,470,936   \n",
              " 1     5/4/2024  USUG12400910              2        545.9     323,703,884   \n",
              " 2    3/19/2024  QZJ842400387              3        538.4     601,309,283   \n",
              " 3    1/12/2023  USSM12209777              4        444.9   2,031,280,633   \n",
              " 4    5/31/2024  USUG12403398              5        423.3     107,034,922   \n",
              " \n",
              "   Spotify Playlist Count Spotify Playlist Reach  ...  SiriusXM Spins  \\\n",
              " 0                 30,716            196,631,588  ...             684   \n",
              " 1                 28,113            174,597,137  ...               3   \n",
              " 2                 54,331            211,607,669  ...             536   \n",
              " 3                269,802            136,569,078  ...           2,182   \n",
              " 4                  7,223            151,469,874  ...               1   \n",
              " \n",
              "   Deezer Playlist Count Deezer Playlist Reach Amazon Playlist Count  \\\n",
              " 0                    62            17,598,718                   114   \n",
              " 1                    67            10,422,430                   111   \n",
              " 2                   136            36,321,847                   172   \n",
              " 3                   264            24,684,248                   210   \n",
              " 4                    82            17,660,624                   105   \n",
              " \n",
              "   Pandora Streams Pandora Track Stations Soundcloud Streams  Shazam Counts  \\\n",
              " 0      18,004,655                 22,931          4,818,457      2,669,262   \n",
              " 1       7,780,028                 28,444          6,623,075      1,118,279   \n",
              " 2       5,022,621                  5,639          7,208,651      5,285,340   \n",
              " 3     190,260,277                203,384                NaN     11,822,942   \n",
              " 4       4,493,884                  7,006            207,179        457,017   \n",
              " \n",
              "   TIDAL Popularity Explicit Track  \n",
              " 0              NaN              0  \n",
              " 1              NaN              1  \n",
              " 2              NaN              0  \n",
              " 3              NaN              0  \n",
              " 4              NaN              1  \n",
              " \n",
              " [5 rows x 29 columns],\n",
              "    Unnamed: 0                track_id                 artists  \\\n",
              " 0           0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n",
              " 1           1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
              " 2           2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
              " 3           3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
              " 4           4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
              " \n",
              "                                           album_name  \\\n",
              " 0                                             Comedy   \n",
              " 1                                   Ghost (Acoustic)   \n",
              " 2                                     To Begin Again   \n",
              " 3  Crazy Rich Asians (Original Motion Picture Sou...   \n",
              " 4                                            Hold On   \n",
              " \n",
              "                    track_name  popularity  duration_ms  explicit  \\\n",
              " 0                      Comedy          73       230666     False   \n",
              " 1            Ghost - Acoustic          55       149610     False   \n",
              " 2              To Begin Again          57       210826     False   \n",
              " 3  Can't Help Falling In Love          71       201933     False   \n",
              " 4                     Hold On          82       198853     False   \n",
              " \n",
              "    danceability  energy  ...  loudness  mode  speechiness  acousticness  \\\n",
              " 0         0.676  0.4610  ...    -6.746     0       0.1430        0.0322   \n",
              " 1         0.420  0.1660  ...   -17.235     1       0.0763        0.9240   \n",
              " 2         0.438  0.3590  ...    -9.734     1       0.0557        0.2100   \n",
              " 3         0.266  0.0596  ...   -18.515     1       0.0363        0.9050   \n",
              " 4         0.618  0.4430  ...    -9.681     1       0.0526        0.4690   \n",
              " \n",
              "    instrumentalness  liveness  valence    tempo  time_signature  track_genre  \n",
              " 0          0.000001    0.3580    0.715   87.917               4     acoustic  \n",
              " 1          0.000006    0.1010    0.267   77.489               4     acoustic  \n",
              " 2          0.000000    0.1170    0.120   76.332               4     acoustic  \n",
              " 3          0.000071    0.1320    0.143  181.740               3     acoustic  \n",
              " 4          0.000000    0.0829    0.167  119.949               4     acoustic  \n",
              " \n",
              " [5 rows x 21 columns])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pip install pandas\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "PROJECT_ROOT = Path(r\"C:/Users/Muhammad Bilal/Desktop/dataviz 1\")\n",
        "DATA_DIR = PROJECT_ROOT / \"viz\" / \"data\" / \"raw\"\n",
        "PROCESSED_DIR = PROJECT_ROOT / \"viz\" / \"data\" / \"processed\"\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"Exists:\", DATA_DIR.exists())\n",
        "print(\"Contents:\", list(DATA_DIR.iterdir()))\n",
        "print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n",
        "\n",
        "# Helper to read CSVs with basic options\n",
        "\n",
        "def read_csv_safe(path: Path, **kwargs) -> pd.DataFrame:\n",
        "    print(f\"Reading {path} ...\")\n",
        "    return pd.read_csv(path, low_memory=False, **kwargs)\n",
        "\n",
        "charts_path = DATA_DIR / \"charts.csv\" \n",
        "most_streamed_path = DATA_DIR / \"Most Streamed Spotify Songs 2024.csv\"\n",
        "tracks_path = DATA_DIR / \"tracks.csv\"\n",
        "\n",
        "charts_head = read_csv_safe(charts_path, nrows=5)\n",
        "most_streamed_head = read_csv_safe(most_streamed_path, nrows=5, encoding=\"latin1\", encoding_errors=\"replace\")\n",
        "tracks_head = read_csv_safe(tracks_path, nrows=5)\n",
        "\n",
        "charts_head, most_streamed_head, tracks_head"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87be704e",
      "metadata": {},
      "source": [
        "## 2. Cleaning plan (high level)\n",
        "\n",
        "We'll implement the cleaning/processing in later cells, roughly:\n",
        "\n",
        "- **charts.csv**: parse dates, ensure numeric `rank`/`streams`, possibly filter by years/regions, and aggregate for chart-friendly tables.\n",
        "- **Most Streamed Spotify Songs 2024.csv**: coerce numeric columns (remove commas), parse `Release Date`, and create tidy tables for rankings and platform mixes.\n",
        "- **tracks.csv**: coerce booleans (`explicit`), ensure numeric audio features, and prepare per-genre summaries and feature matrices.\n",
        "\n",
        "We'll also consider writing out smaller, analysis-ready files (e.g. `processed_charts.parquet`, `processed_most_streamed.parquet`, `processed_tracks.parquet`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3663dd4",
      "metadata": {},
      "source": [
        "## 3. `charts.csv` cleaning\n",
        "\n",
        "Goals:\n",
        "- Parse the `date` column.\n",
        "- Ensure `rank` and `streams` are numeric.\n",
        "- Optionally filter to a max rank (e.g. top 50) and a minimum date to keep file sizes manageable.\n",
        "- Write out a processed CSV we can load quickly for visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a01f061d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote cleaned charts to C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\data\\processed\\charts_clean_top50.csv\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "WindowsPath('C:/Users/Muhammad Bilal/Desktop/dataviz 1/viz/data/processed/charts_clean_top50.csv')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_charts(\n",
        "    in_path: Path = charts_path,\n",
        "    out_path: Path | None = None,\n",
        "    max_rank: int | None = 50,\n",
        "    min_date: str | None = None,\n",
        "    chunksize: int = 250_000,\n",
        ") -> Path:\n",
        "    \"\"\"Stream-clean charts.csv into a smaller, tidy CSV.\n",
        "\n",
        "    - Reads in chunks to avoid loading the 3GB file into memory.\n",
        "    - Parses dates and numeric fields.\n",
        "    - Optionally keeps only rows with rank <= max_rank and date >= min_date.\n",
        "    \"\"\"\n",
        "\n",
        "    if out_path is None:\n",
        "        out_path = PROCESSED_DIR / \"charts_clean_top50.csv\"\n",
        "\n",
        "    # Remove existing file if present so we can append cleanly\n",
        "    if out_path.exists():\n",
        "        out_path.unlink()\n",
        "\n",
        "    parse_dates = [\"date\"]\n",
        "\n",
        "    for chunk in pd.read_csv(in_path, chunksize=chunksize, parse_dates=parse_dates):\n",
        "        # Ensure dtypes\n",
        "        chunk[\"rank\"] = pd.to_numeric(chunk[\"rank\"], errors=\"coerce\")\n",
        "        chunk[\"streams\"] = pd.to_numeric(chunk[\"streams\"], errors=\"coerce\")\n",
        "\n",
        "        if min_date is not None:\n",
        "            chunk = chunk[chunk[\"date\"] >= pd.to_datetime(min_date)]\n",
        "\n",
        "        if max_rank is not None:\n",
        "            chunk = chunk[chunk[\"rank\"] <= max_rank]\n",
        "\n",
        "        # Drop rows that failed conversion\n",
        "        chunk = chunk.dropna(subset=[\"rank\", \"streams\", \"date\"])\n",
        "\n",
        "        # Append to output CSV\n",
        "        header = not out_path.exists()\n",
        "        chunk.to_csv(out_path, mode=\"a\", index=False, header=header)\n",
        "\n",
        "    print(\"Wrote cleaned charts to\", out_path)\n",
        "    return out_path\n",
        "\n",
        "\n",
        "clean_charts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e025c20",
      "metadata": {},
      "source": [
        "## 4. `Most Streamed Spotify Songs 2024.csv` cleaning\n",
        "\n",
        "Goals:\n",
        "- Handle non-UTF8 encoding.\n",
        "- Parse `Release Date` as a proper datetime.\n",
        "- Convert key numeric columns with commas (e.g. `Spotify Streams`) into numeric types.\n",
        "- Save a compact, analysis-ready version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2f79b7b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could not write parquet (install pyarrow or fastparquet if needed): Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
            "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
            "Trying to import the above resulted in these errors:\n",
            " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
            " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n",
            "Wrote cleaned most-streamed data to C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\data\\processed\\most_streamed_2024_clean.csv\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(WindowsPath('C:/Users/Muhammad Bilal/Desktop/dataviz 1/viz/data/processed/most_streamed_2024_clean.csv'),\n",
              " None)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_most_streamed(in_path: Path = most_streamed_path, out_stem: str = \"most_streamed_2024_clean\") -> tuple[Path, Path | None]:\n",
        "    \"\"\"Clean the 2024 most-streamed dataset and write CSV (+ optional parquet).\n",
        "\n",
        "    - Uses latin1 + encoding_errors=\"replace\" to avoid Unicode issues.\n",
        "    - Parses release dates.\n",
        "    - Converts selected numeric columns that contain commas into real numbers.\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(in_path, encoding=\"latin1\", encoding_errors=\"replace\", low_memory=False)\n",
        "\n",
        "    # Parse release date\n",
        "    if \"Release Date\" in df.columns:\n",
        "        df[\"Release Date\"] = pd.to_datetime(df[\"Release Date\"], errors=\"coerce\")\n",
        "\n",
        "    # Columns with large numeric values often stored as strings with commas\n",
        "    numeric_like_cols = [\n",
        "        \"Spotify Streams\",\n",
        "        \"Spotify Playlist Count\",\n",
        "        \"Spotify Playlist Reach\",\n",
        "        \"Spotify Popularity\",\n",
        "        \"YouTube Views\",\n",
        "        \"YouTube Likes\",\n",
        "        \"TikTok Posts\",\n",
        "        \"TikTok Likes\",\n",
        "        \"TikTok Views\",\n",
        "        \"Apple Music Playlist Count\",\n",
        "        \"AirPlay Spins\",\n",
        "        \"Deezer Playlist Count\",\n",
        "        \"Deezer Playlist Reach\",\n",
        "        \"Pandora Streams\",\n",
        "        \"Pandora Track Stations\",\n",
        "        \"Soundcloud Streams\",\n",
        "        \"Shazam Counts\",\n",
        "    ]\n",
        "\n",
        "    for col in numeric_like_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = (\n",
        "                df[col]\n",
        "                .astype(str)\n",
        "                .str.replace(\",\", \"\", regex=False)\n",
        "                .replace({\"\": pd.NA})\n",
        "            )\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    # Write outputs\n",
        "    out_csv = PROCESSED_DIR / f\"{out_stem}.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "\n",
        "    out_parquet = PROCESSED_DIR / f\"{out_stem}.parquet\"\n",
        "    try:\n",
        "        df.to_parquet(out_parquet, index=False)\n",
        "    except Exception as e:  # pyarrow/fastparquet may not be installed\n",
        "        print(\"Could not write parquet (install pyarrow or fastparquet if needed):\", e)\n",
        "        out_parquet = None\n",
        "\n",
        "    print(\"Wrote cleaned most-streamed data to\", out_csv)\n",
        "    return out_csv, out_parquet\n",
        "\n",
        "\n",
        "clean_most_streamed()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13328d1",
      "metadata": {},
      "source": [
        "## 5. `tracks.csv` cleaning\n",
        "\n",
        "Goals:\n",
        "- Ensure audio feature columns are numeric.\n",
        "- Coerce `explicit` to a proper boolean.\n",
        "- Save a clean version for feature-space analysis and joining with other datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d8ff3fed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could not write parquet (install pyarrow or fastparquet if needed): Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
            "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
            "Trying to import the above resulted in these errors:\n",
            " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
            " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n",
            "Wrote cleaned tracks data to C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\data\\processed\\tracks_clean.csv\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(WindowsPath('C:/Users/Muhammad Bilal/Desktop/dataviz 1/viz/data/processed/tracks_clean.csv'),\n",
              " None)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_tracks(in_path: Path = tracks_path, out_stem: str = \"tracks_clean\") -> tuple[Path, Path | None]:\n",
        "    \"\"\"Clean the audio-features dataset and write CSV (+ optional parquet).\n",
        "\n",
        "    - Ensures numeric dtypes for audio features and popularity.\n",
        "    - Coerces the `explicit` flag to boolean.\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(in_path, low_memory=False)\n",
        "\n",
        "    # Explicit to boolean\n",
        "    if \"explicit\" in df.columns:\n",
        "        df[\"explicit\"] = df[\"explicit\"].astype(str).str.lower().isin([\"true\", \"1\", \"yes\"])\n",
        "\n",
        "    numeric_cols = [\n",
        "        \"popularity\",\n",
        "        \"duration_ms\",\n",
        "        \"danceability\",\n",
        "        \"energy\",\n",
        "        \"key\",\n",
        "        \"loudness\",\n",
        "        \"mode\",\n",
        "        \"speechiness\",\n",
        "        \"acousticness\",\n",
        "        \"instrumentalness\",\n",
        "        \"liveness\",\n",
        "        \"valence\",\n",
        "        \"tempo\",\n",
        "        \"time_signature\",\n",
        "    ]\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    out_csv = PROCESSED_DIR / f\"{out_stem}.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "\n",
        "    out_parquet = PROCESSED_DIR / f\"{out_stem}.parquet\"\n",
        "    try:\n",
        "        df.to_parquet(out_parquet, index=False)\n",
        "    except Exception as e:\n",
        "        print(\"Could not write parquet (install pyarrow or fastparquet if needed):\", e)\n",
        "        out_parquet = None\n",
        "\n",
        "    print(\"Wrote cleaned tracks data to\", out_csv)\n",
        "    return out_csv, out_parquet\n",
        "\n",
        "\n",
        "clean_tracks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "86a50f1c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Replaced charts_clean_top50.csv with top 50 rows from most_streamed_2024_clean → C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\data\\processed\\charts_clean_top50.csv\n"
          ]
        }
      ],
      "source": [
        "# Overwrite charts_clean_top50.csv with top 50 entries from most_streamed_2024_clean\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Set up paths\n",
        "PROJECT_ROOT = Path(r\"C:/Users/Muhammad Bilal/Desktop/dataviz 1\")\n",
        "PROCESSED_DIR = PROJECT_ROOT / \"viz\" / \"data\" / \"processed\"\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load the cleaned 2024 most-streamed dataset\n",
        "ms_clean = pd.read_csv(PROCESSED_DIR / \"most_streamed_2024_clean.csv\")\n",
        "\n",
        "# Ensure we have a ranking column to sort by (fallback to Spotify Streams if needed)\n",
        "rank_col = \"All Time Rank\" if \"All Time Rank\" in ms_clean.columns else None\n",
        "\n",
        "if rank_col is not None:\n",
        "    # Convert All Time Rank to numeric (handles strings with commas like \"1,000\")\n",
        "    ms_clean[rank_col] = (\n",
        "        ms_clean[rank_col]\n",
        "        .astype(str)\n",
        "        .str.replace(\",\", \"\", regex=False)\n",
        "        .replace({\"\": pd.NA, \"nan\": pd.NA})\n",
        "    )\n",
        "    ms_clean[rank_col] = pd.to_numeric(ms_clean[rank_col], errors=\"coerce\")\n",
        "    \n",
        "    # Sort by numeric rank and take top 50\n",
        "    top50 = ms_clean.sort_values(rank_col, ascending=True, na_position=\"last\").head(50)\n",
        "else:\n",
        "    # Fallback: use Spotify Streams descending if rank column is missing\n",
        "    top50 = ms_clean.sort_values(\"Spotify Streams\", ascending=False).head(50)\n",
        "\n",
        "out_path = PROCESSED_DIR / \"charts_clean_top50.csv\"\n",
        "\n",
        "# Overwrite the existing file with this new top-50 table\n",
        "top50.to_csv(out_path, index=False)\n",
        "print(\"Replaced charts_clean_top50.csv with top 50 rows from most_streamed_2024_clean →\", out_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d77e9d95",
      "metadata": {},
      "source": [
        "## 6. Export data for frontend visualizations\n",
        "\n",
        "Convert cleaned CSVs to JSON files that can be easily loaded in the Next.js app.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a552b9c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported top 50 tracks to C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\public\\data\\top50_tracks.json\n",
            "Exported full most_streamed dataset to C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\public\\data\\most_streamed_2024.json\n",
            "Exported top 1000 tracks by popularity to C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\public\\data\\tracks_sample.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "# Path to public data folder in Next.js app\n",
        "PUBLIC_DATA_DIR = PROJECT_ROOT / \"viz\" / \"public\" / \"data\"\n",
        "PUBLIC_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Helper function to clean NaN values for JSON export\n",
        "def clean_for_json(obj):\n",
        "    \"\"\"Recursively replace NaN, NaT, and other non-JSON-serializable values with null\"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: clean_for_json(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [clean_for_json(item) for item in obj]\n",
        "    elif pd.isna(obj):\n",
        "        return None\n",
        "    elif isinstance(obj, float) and np.isnan(obj):\n",
        "        return None\n",
        "    elif isinstance(obj, pd.Timestamp):\n",
        "        return str(obj)\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Export top 50 as JSON for leaderboard\n",
        "top50_json = top50.to_dict(orient=\"records\")\n",
        "top50_json_clean = clean_for_json(top50_json)\n",
        "with open(PUBLIC_DATA_DIR / \"top50_tracks.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(top50_json_clean, f, indent=2)\n",
        "print(f\"Exported top 50 tracks to {PUBLIC_DATA_DIR / 'top50_tracks.json'}\")\n",
        "\n",
        "# Export full most_streamed dataset (for other visualizations)\n",
        "ms_full = pd.read_csv(PROCESSED_DIR / \"most_streamed_2024_clean.csv\")\n",
        "ms_json = ms_full.to_dict(orient=\"records\")\n",
        "ms_json_clean = clean_for_json(ms_json)\n",
        "with open(PUBLIC_DATA_DIR / \"most_streamed_2024.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(ms_json_clean, f, indent=2)\n",
        "print(f\"Exported full most_streamed dataset to {PUBLIC_DATA_DIR / 'most_streamed_2024.json'}\")\n",
        "\n",
        "# Export tracks data (sample top 1000 by popularity for now to keep file size manageable)\n",
        "tracks_df = pd.read_csv(PROCESSED_DIR / \"tracks_clean.csv\")\n",
        "tracks_top = tracks_df.nlargest(1000, \"popularity\")\n",
        "tracks_json = tracks_top.to_dict(orient=\"records\")\n",
        "tracks_json_clean = clean_for_json(tracks_json)\n",
        "with open(PUBLIC_DATA_DIR / \"tracks_sample.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(tracks_json_clean, f, indent=2)\n",
        "print(f\"Exported top 1000 tracks by popularity to {PUBLIC_DATA_DIR / 'tracks_sample.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b3c54a3",
      "metadata": {},
      "source": [
        "## 7. Export Billboard Hot 100 datasets for frontend\n",
        "\n",
        "Convert the new Billboard datasets to JSON files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4537ad4e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported weekly top 50 to C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\public\\data\\billboard_weekly_top50.json\n",
            "Exported year-end top 50 to C:\\Users\\Muhammad Bilal\\Desktop\\dataviz 1\\viz\\public\\data\\billboard_yearend_top50.json\n"
          ]
        }
      ],
      "source": [
        "# Export Billboard Hot 100 datasets\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Set up paths\n",
        "PROJECT_ROOT = Path(r\"C:/Users/Muhammad Bilal/Desktop/dataviz 1\")\n",
        "PUBLIC_DATA_DIR = PROJECT_ROOT / \"viz\" / \"public\" / \"data\"\n",
        "PUBLIC_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "NEW_DATA_DIR = PROJECT_ROOT / \"viz\" / \"newdata\"\n",
        "\n",
        "# Helper function to clean NaN values for JSON export (same as Cell 12)\n",
        "def clean_for_json(obj):\n",
        "    \"\"\"Recursively replace NaN, NaT, and other non-JSON-serializable values with null\"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: clean_for_json(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [clean_for_json(item) for item in obj]\n",
        "    elif pd.isna(obj):\n",
        "        return None\n",
        "    elif isinstance(obj, float) and np.isnan(obj):\n",
        "        return None\n",
        "    elif isinstance(obj, pd.Timestamp):\n",
        "        return str(obj)\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Export weekly top 50 dataset\n",
        "weekly_df = pd.read_csv(NEW_DATA_DIR / \"option_a_top50_2025.csv\")\n",
        "weekly_df[\"week_date\"] = pd.to_datetime(weekly_df[\"week_date\"], errors=\"coerce\")\n",
        "weekly_json = weekly_df.to_dict(orient=\"records\")\n",
        "weekly_json_clean = clean_for_json(weekly_json)\n",
        "with open(PUBLIC_DATA_DIR / \"billboard_weekly_top50.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(weekly_json_clean, f, indent=2)\n",
        "print(f\"Exported weekly top 50 to {PUBLIC_DATA_DIR / 'billboard_weekly_top50.json'}\")\n",
        "\n",
        "# Export year-end top 50 dataset\n",
        "yearend_df = pd.read_csv(NEW_DATA_DIR / \"derived_yearend_top50_2025.csv\")\n",
        "yearend_json = yearend_df.to_dict(orient=\"records\")\n",
        "yearend_json_clean = clean_for_json(yearend_json)\n",
        "with open(PUBLIC_DATA_DIR / \"billboard_yearend_top50.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(yearend_json_clean, f, indent=2)\n",
        "print(f\"Exported year-end top 50 to {PUBLIC_DATA_DIR / 'billboard_yearend_top50.json'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e85d3dd",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
